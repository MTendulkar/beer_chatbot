
# TalkBeer Bot Trainer

The third step in my chatbot project is to use my processed post data to train a neural net. 

I opted for a "Long Short-Term Memory" architecture. This is a variation of a recurrent neural net, which cycles its output back into itself. I'm still learning about LSTMs, so I don't have an informed thought process here, but my loose understanding here is: 

- I create a neural net whose inputs are the individual vocab words 
- I break the input corpus into batches 
- Each batch is fed into the inputs, which outputs a list of probabilities for each vocab word to follow
- A word is chosen based on the conditional probabilities of the words before it

I've opted to go with a nonlinear activation function (otherwise, the neural net is just a linear combination of all of the inputs, which means you don't need multiple layers; you could abstract it down to 1). I'm using a ReLU function, which is 0 when x <= 0 and linear thereafter.  


## How it Works

This trains on the conversationData.csv file generated by the processor in Step 2. It accepts two optional inputs: 

```
python train_tb_bot_gpu.py --last_epoch value --num_epochs value 
```

If last_epoch > 0, the program knows to restore from its last checkpoint. 

It performs two operations: 

1. It cleans the corpus a bit further (as I mentioned before, I handled some tokens badly, so I had to push that work into this layer. It will be cleaned up in the future). 
2. It creates a Tensorflow session on a GPU-enabled instance and trains the LSTM, saving every 100 epochs. 


## Getting Started

Clone the repo to your local machine. 

### Prerequisites 

Move the conversationData.csv file to the current directory, and run the script. 